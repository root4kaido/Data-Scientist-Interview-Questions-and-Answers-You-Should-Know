## Machine Learning

### Q1: 線形回帰を行うにあたって必要な仮定は何か？これらの仮定のいくつかが違反している場合はどうなる？

4つの仮定が存在する．
1. 線形性．XとYの平均値との関係は直線的である．
2. 等分散性．残差の分散は，Xのどの値に対しても同じである．
3. 独立．観測は互いに独立している．
4. 正規性．Xの任意の固定値について，Yは正規分布である．

これらの仮定の極端な違反は，結果を冗長にする．これらの仮定の小さな違反は，見積りのバイアスや分散を大きくする．

### Q2: 共線性とは？多重共線性とは？どのように対処する？
共線性: 2つの予測変数間の線形関連．
多重共線性: 2つ以上の予測変数が高度に線形に関連している状況である．
これは，独立変数の統計的有意性を損なうので，問題となる．これは必ずしもモデルの精度に大きな影響を与えないかもしれないが、予測の分散に影響を与え、独立変数の解釈の質を低下させる。
独立変数間に多重共線性があるかどうかを判断するために、VIFを使用することができる - 標準的なベンチマークでは、VIFが5より大きい場合、多重共線性が存在する。

VIF統計量とは・・・？
重回帰分析を行なった際に各変数に多重共線性が無いかを調べるための統計量。
各変数の相関係数も多重共線性の指標となりうるが、VIFを調べることによってそのモデルで使用される複数の変数の関係性を考慮することができる。

[RでVIF統計量を算出しよう！ - Qiita](https://qiita.com/DaichiHasegawa/items/d73da347b9ac81254c82)

### Q3: 線形モデルの欠点は？
- 線形関係，多変量正規性，多共線性がないか，またはほとんどないこと，自己相関がないこと，および同種混合性を前提としている．
- 線形モデルは，離散的な結果やバイナリの結果には使えない．
- モデルの柔軟性を変えることはできない．

### Q4: リッジとラッソ回帰とは？それらの違いは？
L1正則化とL2正則化の両方が，学習データのオーバーフィットを減らすために使用される手法．最小二乗法は，二乗残差の合計を最小化し，バイアスは低くても分散が高くなる可能性があります。

L2正則化は，リッジ回帰とも呼ばれ，2乗残差の合計にラムダ，傾きの2乗を加えたものを最小化する．この追加項は，リッジ回帰ペナルティと呼ばれます．これはモデルのバイアスを増加させ、訓練データでの適合を悪くしますが，分散も減少させます．
リッジ回帰のペナルティを傾きの絶対値に置き換えると，LASSO回帰またはL1正則化が得られる．

![\text { Lasso: Loss }=\operatorname{Error}(y, \hat{y})+\lambda \sum_{i=1}^{N}\left|w_{i}\right|](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext+%7B+Lasso%3A+Loss+%7D%3D%5Coperatorname%7BError%7D%28y%2C+%5Chat%7By%7D%29%2B%5Clambda+%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5Cleft%7Cw_%7Bi%7D%5Cright%7C)

![\text { Lidge: Loss}=\operatorname{Error}(y, \hat{y})+\lambda \sum_{i=1}^{N} w_{i}^{2}](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext+%7B+Lidge%3A+Loss%7D%3D%5Coperatorname%7BError%7D%28y%2C+%5Chat%7By%7D%29%2B%5Clambda+%5Csum_%7Bi%3D1%7D%5E%7BN%7D+w_%7Bi%7D%5E%7B2%7D)


### Q5: K最近傍法はどのように機能する？
![0_o4CAE39VRKzAk3NV.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/2869d20a.png)
K-Nearest Neighborsは，分類された点のうち最も近い点を見て新しいサンプルを分類する分類手法で，「K-nearest」と呼ばれる．上の例では，k=1の場合，未分類の点は青い点として分類される．
kの値が低すぎると，外れ値の影響を受ける可能性がある．しかし，高すぎると，わずかな標本数のクラスを見落とすことがある．

### Q6. k平均法において，kを選択するにはどうすればよい？

kの最適値を決定するために使用される一般的な方法であるエルボー法を使用できる．グラフにkの各値の二乗誤差をプロットし，（x軸にkの値，y軸に二乗誤差）歪みが最も減少するポイントがエルボーポイントになる。
![1_j5T-Wt5bjBfq1XEglL4n-Q.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/b83ee7b7.png)

ただし，現実のデータではうまく行かないことも多い．
シルエット分析というのもある．
[k-meansの最適なクラスター数を調べる方法 - Qiita](https://qiita.com/deaikei/items/11a10fde5bb47a2cf2c2)

### Q7: なぜナイーブベイズが"ナイーブ"なのか？
ナイーブベイズは，特徴が互いに相関していないと仮定されるという強い仮定を保持しているため，ナイーブ．一般的には決してそうではない．

[ナイーブベイズを用いたテキスト分類 - 人工知能に関する断創録](https://aidiary.hatenablog.com/entry/20100613/1276389337)

### Q8: SVMのサポートベクターとは？
サポートベクターは，最大マージンの境界にあたるデータ点．
![1_ZJajse9UxH0Te_Mz5g1fqQ.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/058e95b1.png)

### Q9: 決定木におけるプルーニングとは？
プルーニングは，インスタンスを分類する能力をほとんどまたはまったく提供しないツリーのセクションまたはブランチを削除することにより，決定木のサイズを縮小する機械学習および検索アルゴリズムの手法．

### Q10: ランダムフォレストとはなにか？なぜナイーブベイズのほうが良いのか？
ランダムフォレストは，決定木から構築されるアンサンブル学習手法．ランダム・フォレストでは，元のデータのブートストラップされたデータセットを使用して複数の決定木を作成，決定木の各ステップで変数のサブセットをランダムに選択する．次にモデルは，各決定木のすべての予測のモードを選択します，多数決モデルに依存することで，個々の木からの誤差のリスクを低減する．

![0_2jmC3sVOd9uZnHu7.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/39097862.png)
例えば，1，3つ目の決定木を使用した場合0を予測しするが，4つの決定木すべてのモードに頼った場合，予測値は1になります，これがランダムフォレストの力．
ランダムフォレストは，性能が高い，非線形境界をモデル化できる，クロスバリデーションが不要，特徴の重要性を取得できる，他にもいくつかの利点がある．
訓練しやすく，プロセスや結果を理解しやすいという意味では，ナイーブベイズの方が優れている．ランダムフォレストはブラックボックスのように見えることがあるため，実装や理解の面ではNaive Bayesアルゴリズムの方が良いかもしれない．しかし，性能の面では，アンサンブル手法であるため，ランダムフォレストの方が一般的には強い．

### Q11: ランダムフォレストとSVMを比較し，前者の利点は？
ランダムフォレストでは、特徴の重要度を決定することができる．
ランダムフォレストは，SVMよりもはるかに迅速でシンプルな構築が可能．
多クラス分類の問題ではSVMはone-vs-restを必要とするがこれは拡張性が低く，メモリを多く消費する．

### Q12: 50個の小さな決定木と，大きな1つの決定木，どちらがどうして優れている？
つまり，"ランダムフォレストは決定木よりも優れたモデルなのか？"ということ．そして，ランダムフォレストは多くの弱い決定木を使って強い学習モデルを作るアンサンブル法なので，答えはランダムフォレスト．また，ランダムフォレストは，より正確で，よりロバストで，オーバーフィットしにくい．

### Q13: AdaBoostedツリーとGradientBoostedツリーの違いは？
AdaBoostは，ランダムフォレストに似てるが，下記のような違いがある．
- 木の森ではなく，AdaBoostは通常，切り株の森を作ります (切り株とは，1つのノードと2つの葉を持つ木のこと)．
- 各切り株の予測は，最終的な予測において等しく加重されない．トータルエラーが少ない（精度が高い）切り株ほど発言力が高くなる．
- 後続の各切り株は，前の切り株で誤って分類されたサンプルの重要性を強調するため，切り株が作成される順番は重要．

GradientBoostは，複数の木を構築するという意味ではAdaBoostと似ているが，それぞれの木は前の木を基にして構築される．切り株を構築する AdaBoost とは異なり，GradientBoost は通常 8 から 32 枚の葉を持つ木を構築する．
さらに重要なのは，Gradientは決定木の構築方法がAdaBoostと異なる点．GradientBoostは最初に，通常は平均値のような初期予測から始まる．次に，サンプルの残差に基づいて決定木が構築されます．初期予測値＋学習率に残差木の結果を乗じた値を取って新たな予測を行い，その処理を繰り返す．

### Q14: バイアス-分散のトレードオフとは？
推定量のバイアスは，期待値と真の値との差．バイアスが高いモデルは単純化されすぎて，結果としてアンダーフィッティングになる傾向がある．分散は，データとノイズに対するモデルの感度を表す．高い分散を持つモデルは，オーバーフィッティングになる．
したがって，バイアスと分散のトレードオフは，機械学習モデルの特性であり，分散が低いほどバイアスが高くなり，その逆もまた然り．一般的に，誤差が最小になるような，この2つの最適なバランスを見つけることができる．
![0_M54ORShzS5xVCFSl.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/adb34d25.png)

### Q15: ブートストラップサンプリング法とは？また，それが使用される場合の例を述べよ．
技術的に言えば，ブートストラップ・サンプリング法は，近似分布を用いたランダム・サンプリングを用いたリサンプリング法．
ブートストラップサンプリング法は，ランダムフォレストアルゴリズムや他のアンサンブル学習アルゴリズムと同様に不可欠なもの．

### Q16: バギングとブースティングの違いは？
バギングは，ブートストラップ集計とも呼ばれ，同じ学習アルゴリズムの複数のモデルが元のデータセットのブートストラップされたサンプルで学習されるプロセス．その後，ランダムフォレストのように，すべてのモデルの出力に対して投票が行われる．
![0_FtCQQB5C5uGoHCHQ.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/f5700d1a.png)
ブースティングは，個々のモデルが順番に構築され，前のモデルを反復するバギングのバリエーション．具体的には，前のモデルによって誤って分類されたデータポイントは，次のモデルで強調されます．これは，モデルの全体的な精度を向上させるために行われる．
![0_gFDqgDCpUVetPl5p.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/9e63161a.png)
1つ目のモデルが構築されると，2つ目のモデルを訓練するために，2つ目のブートストラップ標本に加えて，誤って分類された/予測された点が取られる．その後，アンサンブルモデル（モデル1と2）をテストデータセットに対して使用し，プロセスを続ける．

### Q17: XGBoostはバイアスと分散のトレードオフをどのように処理するか？
XGBoostは，勾配ブーストアルゴリズムを活用したアンサンブル機械学習アルゴリズム．本質的には，XGBoostはステロイドの袋詰めとブースティング手法のようなもの(は？)．そのため，XGBoostは他のブースティング手法と同様にバイアスや分散を扱うことができると言える．ブースティングは，多くの弱いモデルの加重平均を取ることで，バイアスと分散の両方を減らすアンサンブルメタアルゴリズム．弱い予測に焦点を当ててモデルを反復処理することで，エラー（したがってバイアス）が削減される．同様に，多くの弱いモデルの加重平均を取るので，最終的なモデルは，弱いモデルのそれぞれよりも低い分散を持つ．

### Q:18 クロスバリデーションとは何か？
クロスバリデーションは，基本的にモデルが新しい独立したデータセットでどの程度うまく機能するかを評価するために使用される手法． 相互検証の最も簡単な例は，データをトレーニングデータ，検証データ，テストデータの3つのグループに分割する場合．ここでは，トレーニングデータを使用してモデルを構築し，検証データを使用してハイパーパラメーターを調整し，テストデータを使用します．最終モデルを評価します．

### Q:19 重回帰を使用して予測モデルを生成する必要があると想定する．このモデルを検証する方法を説明せよ．

A) Adjusted R-squared. 
決定係数は，従属変数の分散の割合が独立変数の分散によってどの程度説明されるかを示す測定値．簡単に言えば，係数は傾向を推定するが，R-squaredは最適な線の周りの散布図を表す． ただし，モデルに独立変数を追加するたびに，決定係数の値が常に増加する．したがって，複数の独立変数を含むモデルは，そうでない場合でも，より適切であるように見える場合があります．これが調整済みR²の出番であり，調整済みR²は追加の独立変数ごとに補正し，与えられた各変数が確率によって可能なものを超えてモデルを改善する場合にのみ増加する。

B) Cross-Validation
データを訓練データ，検証データ，テストデータの3つのセットに分割する．

### Q:20 オンライン学習とバッチ学習の違いは何？
バッチ学習は，オフライン学習とも呼ばれ，パターンのグループに渡って学習する．これは，データセットをソースにして，データセット全体に一度にモデルを構築するという，ほとんどの人がよく知っている学習のタイプ．
一方，オンライン学習は，一度に1つの観測データをインジェストするアプローチ．オンライン学習がデータ効率に優れているのは，一度消費したデータが不要になるためで，技術的にはデータを保存する必要がないことを意味する．

### Q:21 欠落している値に対処する方法をいくつか挙げよ．
- null値の行は完全に省略できる．
- null値を中心傾向の測定値（平均・中央値・最頻値）に置き換えるか，新しいカテゴリ（「なし」など）に置き換えることができる．
- 他の変数に基づいてヌル値を予測することができる．例えば，行の重さにヌル値があるが，高さには値がある場合，ヌル値を指定された高さの平均重さに置き換えることができる．
- 最後に，ヌル値を自動的に扱う機械学習モデルを使用している場合は，ヌル値を残すことができる．

### Q:22 欠測データの平均代入は許容できる方法か？理由もつけて答えよ．
平均入力は，特徴の相関を考慮していないため，一般的には悪い習慣である．例えば，年齢とフィットネス・スコアを示した表があり，80 歳の方のフィットネス・スコアが欠けていると想像する．15歳から80歳までの平均フィットネス・スコアを取ったとすると，80歳の方は実際のフィットネス・スコアよりもはるかに高いスコアを持っているように見える．
第二に，平均入力はデータの分散を減少させ，データのバイアスを増大させる．これは，分散が小さいために精度の低いモデルと信頼区間が狭くなる．

### Q:23 混同行列とは何か？
混同行列（エラー行列としても知られる）は，分類モデルの性能を評価するために使用される要約表．予測の正解数と不正解数をカウント値でまとめ，各クラスごとに分解している．

![0_6l26_xWh8zG4gue4.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/1acc3cd0.png)

### Q:23 教師あり学習と教師なし学習とは何？
教師あり学習では，対象変数が既知のラベル付きデータセットで学習する．
教師なし学習は，ラベル付けされた結果を参照することなく，入力データから推論を導き，パターンを見つけるために使用される．

### Q:24 アンサンブル学習とは何か？
アンサンブル学習とは，複数の学習アルゴリズムを併用する手法．その目的は，個々のアルゴリズムを単独で使用する場合よりも高い予測性能を達成することができるため．その例として，ランダムフォレストが挙げられる．

### Q:25 外れ値をどのように特定できる？
- Zスコア/標準偏差
データセットの99.7%のデータが3σの範囲内にあることがわかっている場合，1つの標準偏差の大きさを計算し，それに3を掛け，この範囲外にあるデータ点を特定することができます．同様に，与えられたポイントのzスコアを計算し，それが±3に等しい場合，それは外れ値．
- 四分位間距離（IQR）。
IQRは，ボックスプロットの作成に使用される概念で，外れ値を識別するためにも使用できる．IQRは，第3分位と第1分位の差に相当する．Q1-1.5*IRQより小さいか，Q3 + 1.5*IQRより大きい場合，あるポイントが外れ値であるかどうかを識別することができる．これは、約2.698の標準偏差になる．

他の方法には，DBScanクラスタリング，分離フォレスト，および堅牢なランダムカットフォレストがある．

### Q:26 inlierとは何？
inlierとは，データセットの残りの部分に存在し，異常であったり，エラーであったりするデータ観測のことである．それはデータセットの中にあるので，一般的には外れ値よりも識別が難しく，識別するためには外部データが必要．

### Q:27 外れ値はどのように処理できるか？
1. 外れ値がゴミである場合は，捨てる．
2. 別のモデル，たとえば，非線形モデルは線形モデルとは異なる方法で外れ値を処理する場合がある．
3. データを正規化して範囲を狭めることができる．
4. ランダムフォレストなどの外れ値を説明するアルゴリズムを使用できる．

### Q:28 協調フィルタリングとコンテンツベースのフィルタリングはどのように似ている/違うか？

コンテンツベースのフィルタリングでは，オブジェクトのプロパティを使用して類似した製品を検索する．たとえば，コンテンツベースのフィルタリングを使用して，映画レコメンダーは同じジャンルの映画や，同じ監督が監督した映画を推薦することができる。

協調フィルタリングでは，あなたの行動を他のユーザーと比較し，似たような行動をとるユーザーがあなたに何を推奨するかを決定する．非常に簡単な例を挙げると，あなたがテレビを購入し，他のユーザーがテレビとリクライニングチェアを購入した場合，あなたはリクライニングチェアも推奨されるだろう．

### Q:29 主成分分析とは？ PCAを使用する問題の種類を説明してください．

最も単純な意味では，PCA はより小さな空間（例えば，2次元）に高次元のデータ（例えば，3次元）を投影することを含む．これは、モデル内のすべての元の変数を維持しながら、データの低次元（3次元の代わりに2次元）になる．
PCAは，圧縮の目的で必要なメモリを削減しアルゴリズムを高速化するために使われる．また，可視化の目的で，データを簡単に要約できるようにするために，一般的に使用される．

### Q:30 検証セットとテストセットの違いは？

一般的に，検証セットはモデルのハイパーパラメータを調整するために使用され，テストセットは最終的なモデルを評価するために使用される．

### Q:31 オーバーフィッティングをどのように避けられるか？

知らない人にとって，オーバーフィッティングとは，機能がデータにフィットしすぎるため，新しいデータがモデルに導入されたときに高レベルのエラーが発生してしまうことである．
モデルのオーバーフィットを防ぐ方法はいくつかあります．

- Cross Validation: クロス・バリデーションは、新しい独立したデータセットでモデルがどれだけうまく機能するかを評価するために使用される手法．
- 正則化: モデルが高次数の多項式を持つ場合、オーバーフィッティングが発生する．したがって正則化は，より高次数の多項式にペナルティを与えることで、オーバーフィッティングを軽減する．
- 特徴量を減らす: 入力特徴量の数を減らすだけで、オーバーフィッティングを減らすこともできる．これは，特徴を手動で削除するか，主成分分析と呼ばれる手法を使用して，高次元データ（3次元など）をより小さな空間（2次元など）に投影する．
- アンサンブル学習のテクニック: アンサンブル・テクニックは，多くの弱い学習モデルをバギングやブーストによって強い学習者モデル変えていく．バギングとブーストを通して，これらのテクニックは代替的な対応策に比べてオーバーフィットが少ない傾向にある．

### Q:32 機械学習アルゴリズムを適用する前に，データの整理やデータのクリーニングを行うには，どのような手順があるか？

データラングリングやデータクリーニングの際に実行できる手順はたくさんある．最も一般的な手順のいくつかを以下に示す．

- データのプロファイリング: ほとんどの人は自分のデータセットを理解することから始める．具体的には，.shape でデータセットの形状を調べたり，.describe() で数値変数の説明を調べたりすることができる．
- データの可視化: ヒストグラム，ボックスプロット，散布図を使ってデータを可視化すると，変数間の関係をよりよく理解したり潜在的な外れ値を特定したりするのに便利な場合がある．
- 構文エラー: これには，空白がないことを確認したり，文字のケーシングが一貫していることを確認したり，タイプミスをチェックしたりすることが含まれる．タイプミスをチェックするには .unique() を使用したり，棒グラフを使用したりする．
- 標準化または正規化: 使用するデータセットや使用する機械学習の方法に応じて，異なる変数の異なるスケールがモデルのパフォーマンスに悪影響を与えないように，データを標準化または正規化することが有用．
- ヌル値の処理: ヌル値を扱う方法には，ヌル値を持つ行を完全に削除する，ヌル値を平均値/中央値/モードに置き換える，ヌル値を新しいカテゴリ（例：不明）に置き換える，値を予測する，またはヌル値を扱うことができる機械学習モデルを使用するなど，多くの方法がある．

他には，無関係なデータの削除，重複データの削除，型変換などがある．

### Q:33 バランスの悪い二進分類にはどのように対処すべきか？

- まず，モデルを評価するために使用するメトリクスを考慮する必要がある．例えば，99回の銀行の引き出しが不正ではなく，1回の引き出しが不正だったとする．モデルがすべてのインスタンスを単に「不正ではない」と分類した場合，精度は99%になる．そのため，精度やリコールのようなメトリクスを使用することを検討した方が良い場合がある．

- 不均衡二値分類を改善する別の方法として，少数派クラスを誤分類するコストを増加させる方法がある．このようなペナルティを増加させることにより，モデルは少数派クラスをより正確に分類する必要が出てくる．

- 少数派クラスをオーバーサンプリングするか，多数派クラスをアンダースサンプリングすることで，クラスのバランスを改善することができる．

### Q:34 presicionとrecallの違いは何か

recall: 実際の陽性の何パーセントが正しく識別されたのか

![\text { Recall }=\frac{T P}{T P+F N}](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext+%7B+Recall+%7D%3D%5Cfrac%7BT+P%7D%7BT+P%2BF+N%7D)

presicion: 正の識別のどの割合が実際に正しかったか

![\text { Precision }=\frac{T P}{T P+F P}
](https://render.githubusercontent.com/render/math?math=%5Cdisplaystyle+%5Ctext+%7B+Precision+%7D%3D%5Cfrac%7BT+P%7D%7BT+P%2BF+P%7D%0A)

### Q:35 平均二乗誤差がモデルのパフォーマンスの悪い尺度であるのはなぜか？代わりは？

平均二乗誤差（MSE）は，大きな誤差に比較的大きな重みを与える．そのため，MSEは大きな偏差を強調しすぎる傾向がある．よりロバストな代替手段としてMAE（平均絶対偏差）がある．

### Q:36 偽陽性と偽陰性とは何か．なぜこれらがお互いに重要なのか？偽陽性が偽陰性よりも重要な場合と、偽陰性が偽陽性よりも重要な場合の例を示しなさい．

偽陽性は，状態が存在しないときに，その状態が存在することを誤って識別すること．
偽陰性は，状態が実際に存在しているときに，状態が存在しないことを誤って識別すること．

偽陽性よりも偽陰性の方が重要な場合の例として，がんのスクリーニング検査がある．誰かががんに罹患していると言って，後になってがんではないことに気づくよりも，がんに罹患しているときにがんではないと言う方がはるかに悪い．
これは主観的な議論だが，心理学的には偽陽性の方が偽陰性よりも悪い場合があります．例えば，宝くじが当たったときの偽陽性は、普通の人はどうせ宝くじが当たるとは思っていないため，偽陰性よりも悪い結果になる可能性がある．


### Q:37 適切な変数を選択するために使用される特徴選択方法は何？

特徴選択の方法には、フィルター法とラッパー法の2種類がある．
フィルター法では，線形判別分析，ANOVA，Chi-Squareがある．
ラッパー法では，以下の方法がある．
フォワードセレクション: 一度に1つの特徴をテストし，適切になるまで追加し続ける．
バックワードセレクション: すべての特徴をテストし，それらを削除して，何がより効果的に機能するかを確認する．

### Q:38 基本的なニューラルネットワークがどのように機能するかを簡単に説明せよ．

ニューラルネットワークは，本質的には数学的な方程式のネットワーク．それは，1つまたは複数の入力変数を取り，方程式のネットワークを通過することで，1つまたは複数の出力変数になる．

![0_8nq9-h-5C4eUzjlE.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/311b680d.png)

ニューラルネットワークには，入力層，1つ以上の隠れ層，出力層がある．入力層は，x1, x2, ..., xnで表される1つ以上の特徴変数（または入力変数または独立変数）で構成される．隠れ層は，１つ以上の隠れノードまたは隠れ単位からなる．ノードとは，上図の円の一つである．同様に出力変数は，1つまたは複数の出力単位から構成される．

![0_wzjAxv0Vwvj8ZBVd.png](:storage/3fdf0be8-689a-43d3-a9de-d7d3fec6b450/ba94aba0.png)

冒頭で述べたように，ニューラルネットワークは方程式のネットワークに過ぎない．ニューラルネットワークの各ノードは，線形関数と活性化関数の2つの関数で構成されている．

### Q:39 ReLUが優れた活性化関数であるのはなぜか？

ReLU関数としても知られており，勾配降下をより速く行うため，シグモイド関数やtanh関数よりも優れた活性化関数であることが知られている．シグモイド関数やtanh関数はx (またはz) が非常に大きい場合，傾きが非常に小さく，勾配降下が著しく遅くなる．しかし，ReLU関数の場合はそうではない．

### Q:40 ネットワークで重みはどのように初期化される？

ニューラルネットワークの重みはランダムに初期化しなければならない(MUST)。
もし，すべての重みを同じ値(すなわち，0または1)に初期化した場合，各隠れユニットは全く同じ信号を得ることになります．例えば，すべての重みを0に初期化した場合，すべての隠れユニットは0のシグナルを得ることになる．

### Q:41 学習率の設定が高すぎたり低すぎたりするとどうなるか？

学習率が低すぎる場合，各反復を通して重みの更新が最小限になるため，モデルの学習は非常に遅くなる．よって，最小点に到達するまでに多くの更新が必要になる．
学習率を高く設定しすぎると，重みの大幅な更新による損失関数の望ましくない発散挙動を引き起こし，収束に失敗する可能性がある．

### Q:42 RNNとは？

リカレント・ニューラル・ネットワーク（RNNとも呼ばれる）は，隠れた状態を持ちながら，以前の出力を入力として使用できるようにするニューラル・ネットワークの一種．
一般的には，時系列データや株式市場データなどのデータ中のシーケンスのパターンを認識するために使用される．

### Q:43 活性化関数の役割は？

活性化関数の目的は，ニューロンの出力に非線形性を導入すること．活性化関数は，加重和を計算し，さらにバイアスを加えることで，ニューロンを活性化すべきかどうかを決定する．
